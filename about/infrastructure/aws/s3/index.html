<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>Simple Storage Service - Automated Fiber Quantification</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.1.2, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="../../../../images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="../../../../css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">

<nav role="navigation">
<ul class="summary">
<li>
<a href="../../../.." target="_blank" class="custom-link">Automated Fiber Quantification</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="">
<a href="../../../..">Home</a>
<li class="header">Background</li>

<li>
<a href="../../../../background/diffusion-mri/" class="">Diffusion MRI</a>
</li>

<li>
<a href="../../../../background/machine-learning/" class="">Machine learning</a>
</li>

<li>
<a href="../../../../background/datasets/" class="">Datasets</a>
</li>

<li class="header">About</li>

<li>
<a href="../../../license/" class="">License</a>
</li>

<li>
<a href="../../../funding/" class="">Funding sources</a>
</li>

<li>
<a href="../../../related/" class="">Related projects</a>
</li>

<li>
<a href="../../../projects/" class="">Our projects</a>
</li>

<li>
<a href="../../../example_recobundles_80_hcp/" class="">Example: RecoBundles 80-bundle segmentation</a>
</li>

<li>
<a href="../../../team/" class="">Team and collaborators</a>
</li>

<li>
<a href="#">Infrastructure</a>
<ul>

<li>
<a href="../../jupyterhub/" class="">Jupyterhub</a>
</li>

<li>
<a href="../../installing_python_packages/" class="">Installing Python packages</a>
</li>

<li>
<a href="#">AWS</a>
<ul>

<li>
<a href="../getting_started/" class="">Getting started</a>
</li>

<li>
<a href="../how_to_use/" class="">Guidelines for our projects</a>
</li>

<li>
<a href="./" class="active">Simple Storage Service</a>
</li>
</ul>
</li>
</ul>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">

<section class="normal markdown-section">



<h1 id="using-amazon-web-services-s3">Using Amazon Web Services S3</h1>
<p>Data for our projects is stored on the <a href="https://aws.amazon.com/">Amazon Web Services</a> (AWS)
<a href="https://aws.amazon.com/s3/">Simple Storage Service</a> (S3).</p>
<h2 id="accessing-data-from-hcp">Accessing data from HCP</h2>
<p>The data from the Human Connectome project is provided as part of
<a href="https://aws.amazon.com/opendata/">AWS Open Data program</a>. The
HCP dataset entry in the program is provided <a href="https://registry.opendata.aws/hcp-openaccess/">here</a>.</p>
<p>To access the processed Human Connectome Project data, use the instructions
provided <a href="https://wiki.humanconnectome.org/display/PublicData/How+to+Get+Access+to+the+HCP+OpenAccess+Amazon+S3+Bucket">here</a></p>
<p>To add your HCP credentials to your configuration, you will need to use the
<a href="../getting_started/">command-line interface</a>.</p>
<pre><code>aws configure --profile hcp
</code></pre>
<p>We also have code in pyAFQ that automatically fetches/reads HCP data from S3.</p>
<h2 id="uploading-data-to-s3">Uploading data to S3</h2>
<p>Before uploading data to our S3 storage, please organize it in a 'BIDSish'
format on your local hard-drive. This will look something like this:</p>
<pre><code>|    &lt;study&gt;
|      ├-derivatives
|            ├-&lt;pipeline&gt;
|                ├── sub01
|                │   ├── ses01
|                │   │   ├── anat
|                │   │   │   ├── sub-01_ses-01_aparc+aseg.nii.gz
|                │   │   │   └── sub-01_ses-01_T1.nii.gz
|                │   │   └── dwi
|                │   │       ├── sub-01_ses-01_dwi.bvals
|                │   │       ├── sub-01_ses-01_dwi.bvecs
|                │   │       └── sub-01_ses-01_dwi.nii.gz
|                │   └── ses02
|                │       ├── anat
|                │       │   ├── sub-01_ses-02_aparc+aseg.nii.gz
|                │       │   └── sub-01_ses-02_T1w.nii.gz
|                │       └── dwi
|                │           ├── sub-01_ses-02_dwi.bvals
|                │           ├── sub-01_ses-02_dwi.bvecs
|                │           └── sub-01_ses-02_dwi.nii.gz
|                └── sub02
|                   ├── ses01
|                   │   ├── anat
|                   │       ├── sub-02_ses-01_aparc+aseg.nii.gz
|                   │   │   └── sub-02_ses-01_T1w.nii.gz
|                   │   └── dwi
|                   │       ├── sub-02_ses-01_dwi.bvals
|                   │       ├── sub-02_ses-01_dwi.bvecs
|                   │       └── sub-02_ses-01_dwi.nii.gz
|                   └── ses02
|                       ├── anat
|                       │   ├── sub-02_ses-02_aparc+aseg.nii.gz
|                       │   └── sub-02_ses-02_T1w.nii.gz
|                       └── dwi
|                           ├── sub-02_ses-02_dwi.bvals
|                           ├── sub-02_ses-02_dwi.bvecs
|                           └── sub-02_ses-02_dwi.nii.gz
</code></pre>
<p>Where <code>study</code> will be the name of the study and will also be the name we will
use for the bucket on S3. Instead of <code>pipeline</code> use a name of the preprocessing
pipeline that you used to process the data. For example, you might use <code>vista</code>,
if you processed the data with the vistalab tools or <code>dmriprep</code> if you use
dmriprep.</p>
<p>Here, we will use the command-line interface to upload the data
(see how to <a href="../getting_started/">get started</a> with that).</p>
<p>The command reference for S3 sub-commands is <a href="https://docs.aws.amazon.com/cli/latest/reference/s3/index.html">here</a>.
Specifically, to create a bucket on S3, you will use the <a href="https://docs.aws.amazon.com/cli/latest/reference/s3/mb.html"><code>mb</code></a> sub-sub-command:</p>
<pre><code>aws s3 mb s3://study
</code></pre>
<p>This command should be executed once. Then, uploading the data as a sync operation:</p>
<pre><code>aws s3 sync path/to/study s3://study
</code></pre>
<p>The use of <code>sync</code> (rather than <code>aws s3 cp</code>) means that only new data would be
uploaded (for example, if repeated calls are made).</p>


</section>

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="../../../../js/main.js"></script>
<script src="../../../../js/gitbook.min.js"></script>
<script src="../../../../js/theme.min.js"></script>
</body>
</html>